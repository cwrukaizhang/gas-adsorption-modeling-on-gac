{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "def pred_dataset(file_names, feature_set = ['BET','Vt'] ):\n",
    "    source_path = 'C:/Kai_Zhang/MachineLearning/Unified gas Adsorption/CO2_adsorption/new_data'\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    for file_name in file_names:\n",
    "        temp_data = pd.read_excel(os.path.join(source_path,file_name+'-02-02-2022.xlsx'),skiprows= 1 )\n",
    "        temp_data = temp_data.dropna(axis=0,how = 'any',subset = feature_set)\n",
    "        temp_data = temp_data[temp_data['Pressure']>0.01]\n",
    "        index = list(set(temp_data['Index'].values))\n",
    "        test_index= np.random.choice(index,int(0.2*len(index)),replace=False)\n",
    "        train_x = temp_data.loc[~temp_data['Index'].isin( test_index)]\n",
    "        test_x = temp_data.loc[temp_data['Index'].isin(test_index)]\n",
    "        \n",
    "        train_df = pd.concat([train_df,train_x],axis=0)\n",
    "        test_df = pd.concat([test_df,test_x],axis =0)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bi_find(data,k):\n",
    "        l,r = 0,len(data)-1\n",
    "        while l<r:\n",
    "            mid = (l+r)>>1\n",
    "            if data[mid]>=k:\n",
    "                r = mid\n",
    "            else:\n",
    "                l = mid+1\n",
    "        return l\n",
    "\n",
    "def inter_data(data:pd.DataFrame):\n",
    "    indexs = list(set(data[\"Index\"].values))\n",
    "    x_mat = np.empty((2,100))\n",
    "    y_mat = []\n",
    "    for index in indexs:\n",
    "        temp_df = data[data[\"Index\"]==index]\n",
    "        x_old = temp_df['Pressure'].values\n",
    "        y_old = temp_df['Adsorp(mmol/g)'].values\n",
    "        res = list(set(temp_df['BET'].values.tolist()))\n",
    "        f = interpolate.interp1d(x_old, y_old,'slinear')\n",
    "        \n",
    "        x_new = np.linspace(min(x_old), max(x_old),num=100)\n",
    "        #x_new\n",
    "        \"\"\"\n",
    "        for ele in x_old:\n",
    "            t = bi_find(x_new,ele)\n",
    "\n",
    "            x_new[t] = ele\n",
    "        \n",
    "        if (len(x_new)>150):\n",
    "            x_new = x_new[len(x_new)-150:]\n",
    "         = np.arange(min(x_old), max(x_old), (max(x_old)-min(x_old))/150)\n",
    "        \"\"\"\n",
    "        y_new = f(x_new)\n",
    "        X_feature = np.append(x_new.reshape(-1,1),y_new.reshape(-1,1),axis=1).reshape(2,100).tolist()\n",
    "        x_mat = np.append(x_mat,X_feature,axis=1)\n",
    "        y_mat.append(res)\n",
    "        \n",
    "    return x_mat,np.array(y_mat).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ChemDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float().reshape(-1,2,100)\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index-1], self.y[index-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(np.array(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_df,test_df = pred_dataset(['CO2'])\n",
    "x_train,y_train = inter_data(train_df)\n",
    "x_test,y_test = inter_data(test_df)\n",
    "data_train = ChemDataset(x_train,y_train)\n",
    "train_loader = DataLoader(data_train, batch_size=128, shuffle=True, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=-1)\n",
    "data_test = ChemDataset(x_test,y_test)\n",
    "test_loader = DataLoader(data_test, batch_size=128, shuffle=True, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "class simpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(simpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # 初始化hidden和memory cell参数\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # forward propagate lstm\n",
    "        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 选取最后一个时刻的输出\n",
    "        #print(out[:, -1, :].shape)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "epochs = 100         # 训练整批数据多少次, 为了节约时间, 我们只训练一次\n",
    "batch_size = 64\n",
    "time_step = 2      # rnn 时间步数 / 图片高度\n",
    "input_size = 100     # rnn 每步输入值 / 图片每行像素\n",
    "hidden_size = 200\n",
    "num_layers = 3\n",
    "num_classes = 1\n",
    "lr = 0.0001\n",
    "\n",
    "model = simpleLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [14/14], Loss: 2014425.7500\n",
      "Epoch [6/100], Step [14/14], Loss: 2046254.1250\n",
      "Epoch [11/100], Step [14/14], Loss: 2285053.2500\n",
      "Epoch [16/100], Step [14/14], Loss: 2442652.7500\n",
      "Epoch [21/100], Step [14/14], Loss: 2349966.7500\n",
      "Epoch [26/100], Step [14/14], Loss: 2545992.7500\n",
      "Epoch [31/100], Step [14/14], Loss: 2644849.0000\n",
      "Epoch [36/100], Step [14/14], Loss: 2579762.7500\n",
      "Epoch [41/100], Step [14/14], Loss: 2646106.2500\n",
      "Epoch [46/100], Step [14/14], Loss: 2439876.7500\n",
      "Epoch [51/100], Step [14/14], Loss: 2452932.2500\n",
      "Epoch [56/100], Step [14/14], Loss: 2528634.7500\n",
      "Epoch [61/100], Step [14/14], Loss: 1991328.8750\n",
      "Epoch [66/100], Step [14/14], Loss: 2868645.7500\n",
      "Epoch [71/100], Step [14/14], Loss: 2326652.5000\n",
      "Epoch [76/100], Step [14/14], Loss: 2575630.0000\n",
      "Epoch [81/100], Step [14/14], Loss: 2389509.5000\n",
      "Epoch [86/100], Step [14/14], Loss: 2976437.7500\n",
      "Epoch [91/100], Step [14/14], Loss: 2349288.5000\n",
      "Epoch [96/100], Step [14/14], Loss: 2159757.5000\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, time_step, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error of the model on test images: 2451821.5 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_error =0\n",
    "    count = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, time_step, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_error += criterion(outputs,labels)*len(labels)\n",
    "        count +=len(labels)\n",
    "        \n",
    "    print('Test error of the model on test images: {} '.format(test_error/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding a few more ff layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(simpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        # 初始化hidden和memory cell参数\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # forward propagate lstm\n",
    "        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 选取最后一个时刻的输出\n",
    "        #print(out[:, -1, :].shape)\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "epochs = 150         # 训练整批数据多少次, 为了节约时间, 我们只训练一次\n",
    "batch_size = 64\n",
    "time_step = 2      # rnn 时间步数 / 图片高度\n",
    "input_size = 100     # rnn 每步输入值 / 图片每行像素\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "num_classes = 1\n",
    "lr = 0.0001\n",
    "\n",
    "model = simpleLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [14/14], Loss: 2026825.3750\n",
      "Epoch [6/150], Step [14/14], Loss: 2973216.2500\n",
      "Epoch [11/150], Step [14/14], Loss: 2283426.0000\n",
      "Epoch [16/150], Step [14/14], Loss: 2192192.7500\n",
      "Epoch [21/150], Step [14/14], Loss: 2496417.2500\n",
      "Epoch [26/150], Step [14/14], Loss: 2889245.0000\n",
      "Epoch [31/150], Step [14/14], Loss: 2093203.7500\n",
      "Epoch [36/150], Step [14/14], Loss: 1782558.8750\n",
      "Epoch [41/150], Step [14/14], Loss: 2426504.0000\n",
      "Epoch [46/150], Step [14/14], Loss: 2417032.0000\n",
      "Epoch [51/150], Step [14/14], Loss: 2117300.7500\n",
      "Epoch [56/150], Step [14/14], Loss: 1438259.0000\n",
      "Epoch [61/150], Step [14/14], Loss: 1998714.5000\n",
      "Epoch [66/150], Step [14/14], Loss: 2564795.2500\n",
      "Epoch [71/150], Step [14/14], Loss: 1538279.2500\n",
      "Epoch [76/150], Step [14/14], Loss: 1070405.6250\n",
      "Epoch [81/150], Step [14/14], Loss: 1884080.3750\n",
      "Epoch [86/150], Step [14/14], Loss: 1012900.1875\n",
      "Epoch [91/150], Step [14/14], Loss: 2018664.1250\n",
      "Epoch [96/150], Step [14/14], Loss: 1599124.5000\n",
      "Epoch [101/150], Step [14/14], Loss: 1813637.2500\n",
      "Epoch [106/150], Step [14/14], Loss: 1496032.0000\n",
      "Epoch [111/150], Step [14/14], Loss: 1485297.5000\n",
      "Epoch [116/150], Step [14/14], Loss: 1419307.7500\n",
      "Epoch [121/150], Step [14/14], Loss: 1105783.7500\n",
      "Epoch [126/150], Step [14/14], Loss: 1416728.1250\n",
      "Epoch [131/150], Step [14/14], Loss: 1085130.1250\n",
      "Epoch [136/150], Step [14/14], Loss: 634683.8125\n",
      "Epoch [141/150], Step [14/14], Loss: 1256100.3750\n",
      "Epoch [146/150], Step [14/14], Loss: 808990.3750\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, time_step, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def bi_find(data,k):\n",
    "        l,r = 0,len(data)-1\n",
    "        while l<r:\n",
    "            mid = (l+r)>>1\n",
    "            if data[mid]>=k:\n",
    "                r = mid\n",
    "            else:\n",
    "                l = mid+1\n",
    "        return l\n",
    "\n",
    "x = data.iloc[:,0].values\n",
    "y = data.iloc[:,1].values\n",
    "f = interpolate.interp1d(x, y,'slinear')\n",
    "xnew = np.arange(min(x), max(x), (max(x)-min(x))/200)\n",
    "for ele in x:\n",
    "    t = bi_find(xnew,ele)\n",
    "    xnew[t] =ele\n",
    "\n",
    "ynew = f(xnew)   # use interpolation function returned by `interp1d`\n",
    "plt.plot(x, y, 'o', xnew, ynew, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 2, 1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(2, 16, 1)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [14/14], Loss: 2323749.5000\n",
      "Epoch [6/200], Step [14/14], Loss: 2977628.7500\n",
      "Epoch [11/200], Step [14/14], Loss: 3308356.2500\n",
      "Epoch [16/200], Step [14/14], Loss: 1747488.6250\n",
      "Epoch [21/200], Step [14/14], Loss: 2086414.2500\n",
      "Epoch [26/200], Step [14/14], Loss: 2391811.0000\n",
      "Epoch [31/200], Step [14/14], Loss: 2457477.0000\n",
      "Epoch [36/200], Step [14/14], Loss: 2221360.0000\n",
      "Epoch [41/200], Step [14/14], Loss: 2137192.0000\n",
      "Epoch [46/200], Step [14/14], Loss: 1261712.5000\n",
      "Epoch [51/200], Step [14/14], Loss: 1726338.5000\n",
      "Epoch [56/200], Step [14/14], Loss: 1654463.8750\n",
      "Epoch [61/200], Step [14/14], Loss: 2027749.8750\n",
      "Epoch [66/200], Step [14/14], Loss: 1225112.5000\n",
      "Epoch [71/200], Step [14/14], Loss: 1294942.0000\n",
      "Epoch [76/200], Step [14/14], Loss: 1269464.5000\n",
      "Epoch [81/200], Step [14/14], Loss: 1740168.6250\n",
      "Epoch [86/200], Step [14/14], Loss: 1520866.1250\n",
      "Epoch [91/200], Step [14/14], Loss: 964939.1875\n",
      "Epoch [96/200], Step [14/14], Loss: 979641.0625\n",
      "Epoch [101/200], Step [14/14], Loss: 1577173.1250\n",
      "Epoch [106/200], Step [14/14], Loss: 1136711.0000\n",
      "Epoch [111/200], Step [14/14], Loss: 1467859.6250\n",
      "Epoch [116/200], Step [14/14], Loss: 1407201.6250\n",
      "Epoch [121/200], Step [14/14], Loss: 1084887.3750\n",
      "Epoch [126/200], Step [14/14], Loss: 969892.3750\n",
      "Epoch [131/200], Step [14/14], Loss: 805765.4375\n",
      "Epoch [136/200], Step [14/14], Loss: 909799.5625\n",
      "Epoch [141/200], Step [14/14], Loss: 776745.2500\n",
      "Epoch [146/200], Step [14/14], Loss: 1089103.6250\n",
      "Epoch [151/200], Step [14/14], Loss: 928389.0000\n",
      "Epoch [156/200], Step [14/14], Loss: 779869.1250\n",
      "Epoch [161/200], Step [14/14], Loss: 595448.8750\n",
      "Epoch [166/200], Step [14/14], Loss: 901534.6250\n",
      "Epoch [171/200], Step [14/14], Loss: 726070.8125\n",
      "Epoch [176/200], Step [14/14], Loss: 838863.5625\n",
      "Epoch [181/200], Step [14/14], Loss: 898575.5625\n",
      "Epoch [186/200], Step [14/14], Loss: 827241.6250\n",
      "Epoch [191/200], Step [14/14], Loss: 772977.4375\n",
      "Epoch [196/200], Step [14/14], Loss: 707472.3125\n"
     ]
    }
   ],
   "source": [
    "epochs =200\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, time_step, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward pass\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 2,2)\n",
    "        self.pool = nn.MaxPool2d(1)\n",
    "        self.conv2 = nn.Conv2d(2, 8, 1)\n",
    "        self.fc1 = nn.Linear(792, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [14/14], Loss: 2170740.5000\n",
      "Epoch [6/200], Step [14/14], Loss: 2870496.0000\n",
      "Epoch [11/200], Step [14/14], Loss: 2324477.5000\n",
      "Epoch [16/200], Step [14/14], Loss: 3012561.2500\n",
      "Epoch [21/200], Step [14/14], Loss: 2442342.7500\n",
      "Epoch [26/200], Step [14/14], Loss: 2131799.5000\n",
      "Epoch [31/200], Step [14/14], Loss: 2154735.2500\n",
      "Epoch [36/200], Step [14/14], Loss: 2075319.1250\n",
      "Epoch [41/200], Step [14/14], Loss: 2068151.6250\n",
      "Epoch [46/200], Step [14/14], Loss: 1889858.8750\n",
      "Epoch [51/200], Step [14/14], Loss: 1256015.7500\n",
      "Epoch [56/200], Step [14/14], Loss: 1784232.5000\n",
      "Epoch [61/200], Step [14/14], Loss: 1612620.5000\n",
      "Epoch [66/200], Step [14/14], Loss: 1478216.0000\n",
      "Epoch [71/200], Step [14/14], Loss: 1263379.0000\n",
      "Epoch [76/200], Step [14/14], Loss: 1451782.2500\n",
      "Epoch [81/200], Step [14/14], Loss: 1618556.5000\n",
      "Epoch [86/200], Step [14/14], Loss: 1281440.7500\n",
      "Epoch [91/200], Step [14/14], Loss: 852093.3125\n",
      "Epoch [96/200], Step [14/14], Loss: 1359693.1250\n",
      "Epoch [101/200], Step [14/14], Loss: 1266140.7500\n",
      "Epoch [106/200], Step [14/14], Loss: 1311855.7500\n",
      "Epoch [111/200], Step [14/14], Loss: 848119.0625\n",
      "Epoch [116/200], Step [14/14], Loss: 696304.3125\n",
      "Epoch [121/200], Step [14/14], Loss: 1187578.0000\n",
      "Epoch [126/200], Step [14/14], Loss: 810194.5625\n",
      "Epoch [131/200], Step [14/14], Loss: 763096.8125\n",
      "Epoch [136/200], Step [14/14], Loss: 647624.0000\n",
      "Epoch [141/200], Step [14/14], Loss: 891039.1250\n",
      "Epoch [146/200], Step [14/14], Loss: 916898.8750\n",
      "Epoch [151/200], Step [14/14], Loss: 830222.7500\n",
      "Epoch [156/200], Step [14/14], Loss: 627626.6250\n",
      "Epoch [161/200], Step [14/14], Loss: 875893.8750\n",
      "Epoch [166/200], Step [14/14], Loss: 1101947.2500\n",
      "Epoch [171/200], Step [14/14], Loss: 872443.3125\n",
      "Epoch [176/200], Step [14/14], Loss: 679741.0000\n",
      "Epoch [181/200], Step [14/14], Loss: 583212.4375\n",
      "Epoch [186/200], Step [14/14], Loss: 513987.4375\n",
      "Epoch [191/200], Step [14/14], Loss: 580687.6250\n",
      "Epoch [196/200], Step [14/14], Loss: 546482.0625\n"
     ]
    }
   ],
   "source": [
    "epochs =200\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 1,time_step, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Using keras to build a cnn network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "def pred_dataset(file_names, feature_set = ['BET','Vt'] ):\n",
    "    source_path = 'C:/Kai_Zhang/MachineLearning/Unified gas Adsorption/CO2_adsorption/new_data'\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    for file_name in file_names:\n",
    "        temp_data = pd.read_excel(os.path.join(source_path,file_name+'-02-02-2022.xlsx'),skiprows= 1 )\n",
    "        temp_data = temp_data.dropna(axis=0,how = 'any',subset = feature_set)\n",
    "        temp_data = temp_data[temp_data['Pressure']>0.01]\n",
    "        index = list(set(temp_data['Index'].values))\n",
    "        test_index= np.random.choice(index,int(0.2*len(index)),replace=False)\n",
    "        train_x = temp_data.loc[~temp_data['Index'].isin( test_index)]\n",
    "        test_x = temp_data.loc[temp_data['Index'].isin(test_index)]\n",
    "        \n",
    "        train_df = pd.concat([train_df,train_x],axis=0)\n",
    "        test_df = pd.concat([test_df,test_x],axis =0)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bi_find(data,k):\n",
    "        l,r = 0,len(data)-1\n",
    "        while l<r:\n",
    "            mid = (l+r)>>1\n",
    "            if data[mid]>=k:\n",
    "                r = mid\n",
    "            else:\n",
    "                l = mid+1\n",
    "        return l\n",
    "\n",
    "def inter_data(data:pd.DataFrame):\n",
    "    indexs = list(set(data[\"Index\"].values))\n",
    "    \n",
    "    x_mat = []\n",
    "    y_mat = []\n",
    "    for index in indexs:\n",
    "        temp_df = data[data[\"Index\"]==index]\n",
    "        x_old = temp_df['Pressure'].values\n",
    "        y_old = temp_df['Adsorp(mmol/g)'].values\n",
    "        res = list(set(temp_df['BET'].values.tolist()))\n",
    "        f = interpolate.interp1d(x_old, y_old,'slinear')\n",
    "        \n",
    "        x_new = np.linspace(min(x_old), max(x_old),num=100)\n",
    "        #x_new\n",
    "        \"\"\"\n",
    "        for ele in x_old:\n",
    "            t = bi_find(x_new,ele)\n",
    "\n",
    "            x_new[t] = ele\n",
    "        \n",
    "        if (len(x_new)>150):\n",
    "            x_new = x_new[len(x_new)-150:]\n",
    "         = np.arange(min(x_old), max(x_old), (max(x_old)-min(x_old))/150)\n",
    "        \"\"\"\n",
    "        y_new = f(x_new)\n",
    "        X_feature = np.append(x_new.reshape(-1,1),y_new.reshape(-1,1),axis=1).reshape(2,100).tolist()\n",
    "        x_mat.append(X_feature)\n",
    "        y_mat.append(res)\n",
    "        \n",
    "    return x_mat,np.array(y_mat).reshape(-1,1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1716 1716\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "import  tensorflow as tf\n",
    "import  numpy as np\n",
    "from   tensorflow import keras\n",
    "from   tensorflow.keras import layers,optimizers,losses\n",
    "from   tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "df_train,df_test = pred_dataset(['CO2'])\n",
    "#df.head()\n",
    "\n",
    "images,labels = inter_data(df_train)\n",
    "images1,labels1 = inter_data(df_test)\n",
    "print(len(images),len(labels))\n",
    "#images,images1,labels,labels1 = train_test_split(img,label,test_size=0.2)\n",
    "\n",
    "#from my_load_data import load_data,preprocess\n",
    "\n",
    "batchsz = 128\n",
    "# 创建训练集Datset对象\n",
    "\n",
    "db_train = tf.data.Dataset.from_tensor_slices((tf.cast(images, dtype=tf.float32), tf.convert_to_tensor(labels,dtype=tf.float32)))\n",
    "db_train = db_train.shuffle(1000).batch(batchsz)\n",
    "# 创建验证集Datset对象\n",
    "\n",
    "db_val = tf.data.Dataset.from_tensor_slices((tf.cast(images1, dtype=tf.float32), tf.convert_to_tensor(labels1,dtype=tf.float32)))\n",
    "db_val = db_val.batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (c:\\Users\\kxz231\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:301742)",
      "at S.execute (c:\\Users\\kxz231\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:300732)",
      "at S.start (c:\\Users\\kxz231\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\kxz231\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (c:\\Users\\kxz231\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,Conv1D\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=1, activation=\"relu\", input_shape=(2,100,1)))\n",
    "model.add(Conv2D(32, kernel_size=1, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"relu\"))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "model.fit(db_train, validation_data = db_val, validation_freq = 1, epochs = 100,\n",
    "           )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cded950f7e8b102373b7ffb2d1ae075c531242f5ad58e5bbcdb99f4873d2799c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch_optuna': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
