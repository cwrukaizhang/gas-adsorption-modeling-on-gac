{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def pred_dataset(file_names, feature_set):\n",
    "    source_path = 'C:/Kai_Zhang/MachineLearning/Unified gas Adsorption/CO2_adsorption/new_data'\n",
    "    data_df = pd.DataFrame()\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        temp_data = pd.read_excel(os.path.join(source_path,file_name+'-02-02-2022.xlsx'),skiprows= 1 )\n",
    "        temp_data = temp_data.dropna(axis=0,how = 'any',subset = feature_set)\n",
    "        #train_x,test_x = train_test_split(temp_data,test_size = 0.2)\n",
    "        data_df = pd.concat([data_df,temp_data],axis=0)\n",
    "        #test_df = pd.concat([test_df,test_x],axis =0)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingRegressor,ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor  \n",
    "\n",
    "n_estimators = [50,100,120,150,180,200]\n",
    "\n",
    "# define different models#,\n",
    "models = [\n",
    "    ('ETR',ExtraTreesRegressor(random_state=42,n_jobs=-1)),\\\n",
    "    ('LGBM',LGBMRegressor(n_jobs = -1,random_state = 42)),\\\n",
    "    ('BGLGBM',BaggingRegressor(LGBMRegressor(n_estimators = 200, n_jobs = -1,random_state = 42), random_state=42,n_jobs=-1)),\\\n",
    "    ]\n",
    "# set search parameters grid for different models\n",
    "para_grids = { \n",
    "    'ETR':{'n_estimators':n_estimators},\\\n",
    "    'LGBM':{'num_leaves':[10,20,30,50],'learning_rate': [0.05,0.1,0.5,1],'n_estimators':n_estimators},\\\n",
    "    'BGLGBM':{'n_estimators':[10,30,50]},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,cross_validate,GroupKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from  sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def model_CV(train_x,train_y,groups,model,para_grid):\n",
    "\n",
    "    out_cv = GroupKFold(n_splits = 5)\n",
    "    result = GridSearchCV(model,para_grid,cv= out_cv.get_n_splits(groups =groups),\n",
    "    scoring='neg_mean_squared_error', return_train_score=True,n_jobs=-1)\n",
    "    result.fit(train_x,train_y)\n",
    "    model_refit =model.set_params(**result.best_params_)\n",
    "    train_cv = cross_validate(model_refit,train_x,train_y,groups = groups,cv =out_cv,scoring = ('r2', 'neg_mean_squared_error'))\n",
    "    train_mse_cv = -train_cv['test_neg_mean_squared_error'].mean()\n",
    "    train_r2_cv = train_cv['test_r2'].mean()\n",
    "    return [train_r2_cv,train_mse_cv],result.best_params_\n",
    "\n",
    "# model evaluation\n",
    "def model_eval(model,test_x,test_y):\n",
    "      \n",
    "    test_pre = model.predict(test_x)\n",
    "    test_r2 = r2_score(test_pre,test_y)\n",
    "    test_mse = mean_squared_error(test_y,test_pre)\n",
    "    return test_r2,test_mse\n",
    "\n",
    "# comparing different models\n",
    "def model_comparison(model_list,para_grids,feature_list,gas_list):\n",
    "    gas_list = gas_list \n",
    "    input_feature = feature_list\n",
    "    output = ['Adsorp(mmol/g)']\n",
    "    result_total = []\n",
    "\n",
    "    for gas in gas_list:\n",
    "            #train_df_com = train_df[train_df['Label']==gas]\n",
    "            train_df_com = train_df\n",
    "            test_df_com = test_df[test_df['Label']==gas]\n",
    "            train_x = train_df_com[input_feature]\n",
    "            test_x = test_df_com[input_feature]\n",
    "            train_y = train_df_com[output].values\n",
    "            test_y = test_df_com[output].values\n",
    "            groups = train_df_com['Index']\n",
    "            train_x, train_y, groups = shuffle(train_x, train_y, groups, random_state=42)\n",
    "            print(f'With big set and the total training records is {len(train_df_com)}')\n",
    "            for model_name, model in model_list:\n",
    "\n",
    "                result, best_param = model_CV(train_x,train_y.squeeze(),groups,model,para_grids[model_name])\n",
    "                model_refit = model.set_params(**best_param)\n",
    "                model_refit.fit(train_x,train_y.squeeze())\n",
    "                test_r2,test_mse = model_eval(model_refit,test_x,test_y.squeeze()) \n",
    "                result_total.append([gas,model_name+'_separate',result[0],result[1],-1,-1, test_r2,test_mse,best_param])\n",
    "                \n",
    "                print('Dataset: {:s}, Algorithm: {:s}, Test_r2: {:.2f}, Test_error: {:.2f}'.format(gas,model_name+'_with_big_set',test_r2,test_mse))\n",
    "\n",
    "            train_df_com = train_df[train_df['Label']==gas]\n",
    "            test_df_com = test_df[test_df['Label']==gas]\n",
    "            train_x = train_df_com[input_feature]\n",
    "            test_x = test_df_com[input_feature]\n",
    "            train_y = train_df_com[output].values\n",
    "            test_y = test_df_com[output].values\n",
    "            groups = train_df_com['Index']\n",
    "            train_x, train_y, groups = shuffle(train_x, train_y, groups, random_state=42)\n",
    "            print(f'With no big set and the total training records is {len(train_df_com)}')\n",
    "            for model_name, model in model_list:\n",
    "\n",
    "                result, best_param = model_CV(train_x,train_y.squeeze(),groups,model,para_grids[model_name])\n",
    "                model_refit = model.set_params(**best_param)\n",
    "                model_refit.fit(train_x,train_y.squeeze())\n",
    "                test_r2,test_mse = model_eval(model_refit,test_x,test_y.squeeze()) \n",
    "                result_total.append([gas,model_name+'_separate',result[0],result[1],-1,-1, test_r2,test_mse,best_param])\n",
    "                print('Dataset: {:s}, Algorithm: {:s}, Test_r2: {:.2f}, Test_error: {:.2f}'.format(gas,model_name+'_no_big_set',test_r2,test_mse))\n",
    "          \n",
    "    return result_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_split(data_df,target,additional):\n",
    "    #np.random.seed(42)\n",
    "    temp_data = data_df[data_df[\"Label\"]==target]\n",
    "    index = list(set(temp_data['Index'].values))\n",
    "    temp_data,_ = shuffle(temp_data, temp_data['Index'].values, random_state=42)\n",
    "    #print(len(index))\n",
    "    test_index= np.random.choice(index,int(0.2*len(index)),replace=False)\n",
    "    train_df = temp_data.loc[~temp_data['Index'].isin( test_index)]\n",
    "    test_df = temp_data.loc[temp_data['Index'].isin(test_index)]\n",
    "    index_len = len(list(set(train_df['Index'].values)))\n",
    "\n",
    "    for gas in additional:\n",
    "        temp_data = data_df[data_df[\"Label\"]==gas]\n",
    "        temp_index = list(set(temp_data['Index'].values))\n",
    "        selected_index =  np.random.choice(temp_index,100,replace=False) # for CO2 and Methane only\n",
    "        temp_train = temp_data.loc[temp_data['Index'].isin(selected_index)]\n",
    "        train_df = pd.concat([train_df,temp_train])\n",
    "        \"\"\"\n",
    "        if len(temp_index)<index_len*6:\n",
    "            train_df = pd.concat([train_df,temp_data])\n",
    "        else:\n",
    "            selected_index =  np.random.choice(temp_index,388,replace=False) # for CO2 and Methane only\n",
    "            temp_train = temp_data.loc[temp_data['Index'].isin(selected_index)]\n",
    "            train_df = pd.concat([train_df,temp_train])\n",
    "        \"\"\"\n",
    "    groups = train_df['Index'].values\n",
    "    train_df, groups = shuffle(train_df, groups, random_state=42)\n",
    "    \n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as nrd\n",
    "from sklearn.decomposition import KernelPCA,SparsePCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from  collections import Counter\n",
    "\n",
    "def bgk_pca(train_df,test_df): \n",
    "    #np.random.RandomState(42)# using major voting approach to find the outliers in the test dataset\n",
    "    test_index = list(set(test_df[\"Index\"].values))\n",
    "    #len_test = len(test_index)\n",
    "    train_index = list(set(train_df[\"Index\"].values))\n",
    "    #total_feature = [\"Index\",'V','L','BET','Vt','Temp(K)']\n",
    "    total_feature = [\"Index\",'V','L','BET','Vt','Temp(K)']\n",
    "    #pca_feature =  ['V','L','BET','Vt','Temp(K)']\n",
    "    pca_feature =  ['V','L','BET','Vt','Temp(K)']\n",
    "    num_feature = len(pca_feature)\n",
    "    removed_index = []\n",
    "    res = []\n",
    "    iters = 10\n",
    "    multi_mse = []\n",
    "    for i in range(iters):\n",
    "        mses = []\n",
    "        train_selected = train_df[train_df[\"Index\"].isin(nrd.choice(train_index,int(len(train_index)*0.75),replace=False))] # modified here change fixed len to a the fraction of the len of the training dataset.\n",
    "        data = pd.concat([test_df,train_selected])\n",
    "        sub_data = data[total_feature].drop_duplicates()\n",
    "        sub_data_scalered = MinMaxScaler().fit_transform(sub_data[pca_feature].values)\n",
    "        \n",
    "        \"\"\"adding lines to determine the number of components to achieve 0.99 threshold\"\"\"\n",
    "        kernel_pca = KernelPCA(kernel='poly',max_iter =100000,n_jobs =-1,gamma=1e-2,fit_inverse_transform=True,random_state=42)\n",
    "        kpca_transform = kernel_pca.fit_transform(sub_data_scalered.reshape(num_feature,-1))\n",
    "        explained_variance = np.var(kpca_transform, axis=0)\n",
    "        explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "        cumu_variance = np.cumsum(explained_variance_ratio)\n",
    "        n_components = np.where(cumu_variance>0.99)[0][0]+1\n",
    "        kernel_pca = KernelPCA(n_components=n_components,kernel='poly',max_iter =100000,n_jobs =-1,gamma=1e-2,fit_inverse_transform=True,random_state=42)\n",
    "        \"\"\" The end of adding new lines\"\"\"\n",
    "\n",
    "        #kernel_pca = KernelPCA(kernel='poly',max_iter =100000,n_jobs =-1,gamma=1e-2,fit_inverse_transform=True,random_state=42)\n",
    "        sub_data_transformed = kernel_pca.fit_transform(sub_data_scalered.reshape(num_feature,-1))\n",
    "        reconstructed = kernel_pca.inverse_transform(sub_data_transformed.reshape(num_feature,-1))\n",
    "        for i in range(len(sub_data_scalered)):\n",
    "            mses.append(mean_squared_error(sub_data_scalered[i],reconstructed.reshape(-1,num_feature)[i]))\n",
    "            df_mse = pd.DataFrame(mses,columns = ['MSE'])\n",
    "        df_mse['Indexs'] = sub_data[\"Index\"].drop_duplicates().values\n",
    "        mean_mse = df_mse[\"MSE\"].mean()\n",
    "        std_mse = df_mse['MSE'].std()\n",
    "        test_mse_df = df_mse[df_mse[\"Indexs\"].isin(test_index)]\n",
    "        outlier_index = test_mse_df[test_mse_df[\"MSE\"]>3*std_mse+mean_mse][\"Indexs\"].values.tolist()\n",
    "        removed_index.extend(outlier_index)\n",
    "        multi_mse.append(mean_mse)\n",
    "\n",
    "    counter = Counter(removed_index)\n",
    "    thresh = int(0.7*iters)\n",
    "    for key,values in counter.most_common():\n",
    "        if values>=thresh:\n",
    "            res.append(key)\n",
    "            \n",
    "        if values<thresh: break\n",
    "\n",
    "    return np.mean(multi_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of test datasets: 310\n",
      "With big set and the total training records is 2429\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9565, Test_error: 0.7840\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9682, Test_error: 0.6181\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9689, Test_error: 0.5882\n",
      "With no big set and the total training records is 1257\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9644, Test_error: 0.6620\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9581, Test_error: 0.8445\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9704, Test_error: 0.5830\n",
      "recons_mse: 0.0728, recons_mse_added_other: 0.0471\n",
      "The length of test datasets: 301\n",
      "With big set and the total training records is 2246\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9496, Test_error: 0.6273\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9525, Test_error: 0.6379\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9602, Test_error: 0.5240\n",
      "With no big set and the total training records is 1266\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9567, Test_error: 0.5448\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9482, Test_error: 0.7428\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9476, Test_error: 0.7170\n",
      "recons_mse: 0.0717, recons_mse_added_other: 0.0529\n",
      "The length of test datasets: 309\n",
      "With big set and the total training records is 2349\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9591, Test_error: 0.7997\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9448, Test_error: 1.1040\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9574, Test_error: 0.8070\n",
      "With no big set and the total training records is 1258\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9550, Test_error: 0.9151\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9575, Test_error: 0.8931\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9522, Test_error: 0.9758\n",
      "recons_mse: 0.0733, recons_mse_added_other: 0.0612\n",
      "The length of test datasets: 337\n",
      "With big set and the total training records is 2371\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9617, Test_error: 0.7341\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9701, Test_error: 0.5990\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9786, Test_error: 0.4461\n",
      "With no big set and the total training records is 1230\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9657, Test_error: 0.6807\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9823, Test_error: 0.3807\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9787, Test_error: 0.4496\n",
      "recons_mse: 0.0723, recons_mse_added_other: 0.0468\n",
      "The length of test datasets: 352\n",
      "With big set and the total training records is 2223\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.7418, Test_error: 2.3662\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.8867, Test_error: 1.2841\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.8668, Test_error: 1.4848\n",
      "With no big set and the total training records is 1215\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.7566, Test_error: 2.8544\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.7677, Test_error: 4.3154\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.7590, Test_error: 4.2661\n",
      "recons_mse: 0.0723, recons_mse_added_other: 0.0513\n",
      "The length of test datasets: 304\n",
      "With big set and the total training records is 2327\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9292, Test_error: 0.9556\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9193, Test_error: 1.2239\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9396, Test_error: 0.8426\n",
      "With no big set and the total training records is 1263\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9363, Test_error: 0.9013\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9208, Test_error: 1.1383\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9471, Test_error: 0.7639\n",
      "recons_mse: 0.0706, recons_mse_added_other: 0.0604\n",
      "The length of test datasets: 344\n",
      "With big set and the total training records is 2332\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9325, Test_error: 1.3739\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9567, Test_error: 0.9772\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9331, Test_error: 1.3991\n",
      "With no big set and the total training records is 1223\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9373, Test_error: 1.2987\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9494, Test_error: 1.1644\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9336, Test_error: 1.5106\n",
      "recons_mse: 0.0723, recons_mse_added_other: 0.0503\n",
      "The length of test datasets: 310\n",
      "With big set and the total training records is 2268\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9461, Test_error: 1.1171\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9786, Test_error: 0.5352\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9678, Test_error: 0.7049\n",
      "With no big set and the total training records is 1257\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9496, Test_error: 1.0486\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9690, Test_error: 0.7243\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9634, Test_error: 0.8148\n",
      "recons_mse: 0.0753, recons_mse_added_other: 0.0540\n",
      "The length of test datasets: 332\n",
      "With big set and the total training records is 2285\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9494, Test_error: 0.7441\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9361, Test_error: 0.8527\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9075, Test_error: 1.1415\n",
      "With no big set and the total training records is 1235\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9493, Test_error: 0.7738\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9137, Test_error: 1.9846\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9547, Test_error: 0.7566\n",
      "recons_mse: 0.0729, recons_mse_added_other: 0.0578\n",
      "The length of test datasets: 317\n",
      "With big set and the total training records is 2398\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9546, Test_error: 1.0182\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9650, Test_error: 0.8138\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9592, Test_error: 0.9289\n",
      "With no big set and the total training records is 1250\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9559, Test_error: 1.0042\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9756, Test_error: 0.5837\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9572, Test_error: 0.9993\n",
      "recons_mse: 0.0720, recons_mse_added_other: 0.0541\n",
      "The length of test datasets: 373\n",
      "With big set and the total training records is 2185\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9648, Test_error: 0.4575\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9679, Test_error: 0.4552\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9548, Test_error: 0.5934\n",
      "With no big set and the total training records is 1194\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9678, Test_error: 0.4335\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9577, Test_error: 0.5826\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9604, Test_error: 0.5235\n",
      "recons_mse: 0.0705, recons_mse_added_other: 0.0537\n",
      "The length of test datasets: 303\n",
      "With big set and the total training records is 2308\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9441, Test_error: 0.9703\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9545, Test_error: 0.8079\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9520, Test_error: 0.8362\n",
      "With no big set and the total training records is 1264\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9456, Test_error: 0.9025\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9502, Test_error: 0.8714\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9433, Test_error: 0.9471\n",
      "recons_mse: 0.0735, recons_mse_added_other: 0.0616\n",
      "The length of test datasets: 346\n",
      "With big set and the total training records is 2375\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9372, Test_error: 1.3766\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9272, Test_error: 1.5970\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9180, Test_error: 1.6884\n",
      "With no big set and the total training records is 1221\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9422, Test_error: 1.2873\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9558, Test_error: 0.9552\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9435, Test_error: 1.1953\n",
      "recons_mse: 0.0738, recons_mse_added_other: 0.0567\n",
      "The length of test datasets: 345\n",
      "With big set and the total training records is 2202\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9480, Test_error: 0.7216\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9571, Test_error: 0.5659\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9526, Test_error: 0.5928\n",
      "With no big set and the total training records is 1222\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9410, Test_error: 0.8099\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9561, Test_error: 0.5654\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9391, Test_error: 0.7688\n",
      "recons_mse: 0.0749, recons_mse_added_other: 0.0547\n",
      "The length of test datasets: 310\n",
      "With big set and the total training records is 2307\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9218, Test_error: 1.0030\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9187, Test_error: 0.9959\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9370, Test_error: 0.8047\n",
      "With no big set and the total training records is 1257\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9218, Test_error: 0.9662\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9040, Test_error: 1.2939\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9185, Test_error: 1.1103\n",
      "recons_mse: 0.0734, recons_mse_added_other: 0.0611\n",
      "The length of test datasets: 346\n",
      "With big set and the total training records is 2355\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9393, Test_error: 0.8228\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9747, Test_error: 0.3737\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9506, Test_error: 0.6544\n",
      "With no big set and the total training records is 1221\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9338, Test_error: 0.8789\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9665, Test_error: 0.4407\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9632, Test_error: 0.4605\n",
      "recons_mse: 0.0725, recons_mse_added_other: 0.0566\n",
      "The length of test datasets: 279\n",
      "With big set and the total training records is 2271\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9200, Test_error: 1.1986\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9652, Test_error: 0.6378\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9600, Test_error: 0.7100\n",
      "With no big set and the total training records is 1288\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9144, Test_error: 1.2673\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9506, Test_error: 0.9955\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9480, Test_error: 0.9521\n",
      "recons_mse: 0.0749, recons_mse_added_other: 0.0563\n",
      "The length of test datasets: 348\n",
      "With big set and the total training records is 2279\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9610, Test_error: 0.4155\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9551, Test_error: 0.5735\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9606, Test_error: 0.4501\n",
      "With no big set and the total training records is 1219\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9687, Test_error: 0.3697\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9600, Test_error: 0.4902\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9741, Test_error: 0.2840\n",
      "recons_mse: 0.0722, recons_mse_added_other: 0.0628\n",
      "The length of test datasets: 320\n",
      "With big set and the total training records is 2380\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.8840, Test_error: 1.6266\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.8992, Test_error: 1.6425\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9069, Test_error: 1.4410\n",
      "With no big set and the total training records is 1247\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.8787, Test_error: 1.6952\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.8996, Test_error: 1.5487\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9077, Test_error: 1.4056\n",
      "recons_mse: 0.0739, recons_mse_added_other: 0.0442\n",
      "The length of test datasets: 285\n",
      "With big set and the total training records is 2291\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9319, Test_error: 1.0720\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9263, Test_error: 1.1491\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9174, Test_error: 1.2375\n",
      "With no big set and the total training records is 1282\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9414, Test_error: 0.9313\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9002, Test_error: 1.5915\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9332, Test_error: 1.0148\n",
      "recons_mse: 0.0712, recons_mse_added_other: 0.0557\n",
      "The length of test datasets: 309\n",
      "With big set and the total training records is 2284\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9636, Test_error: 0.5574\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9777, Test_error: 0.3846\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9799, Test_error: 0.3327\n",
      "With no big set and the total training records is 1258\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9655, Test_error: 0.5392\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9815, Test_error: 0.3341\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9791, Test_error: 0.3541\n",
      "recons_mse: 0.0741, recons_mse_added_other: 0.0611\n",
      "The length of test datasets: 320\n",
      "With big set and the total training records is 2394\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9480, Test_error: 0.9665\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9795, Test_error: 0.3809\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9789, Test_error: 0.3909\n",
      "With no big set and the total training records is 1247\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9533, Test_error: 0.8469\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9691, Test_error: 0.5828\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9761, Test_error: 0.4547\n",
      "recons_mse: 0.0719, recons_mse_added_other: 0.0441\n",
      "The length of test datasets: 319\n",
      "With big set and the total training records is 2277\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9350, Test_error: 1.2688\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9586, Test_error: 0.9242\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9501, Test_error: 1.0439\n",
      "With no big set and the total training records is 1248\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9374, Test_error: 1.2142\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9630, Test_error: 0.8416\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9501, Test_error: 1.0523\n",
      "recons_mse: 0.0730, recons_mse_added_other: 0.0543\n",
      "The length of test datasets: 286\n",
      "With big set and the total training records is 2339\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9590, Test_error: 0.5548\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9679, Test_error: 0.4171\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9675, Test_error: 0.4515\n",
      "With no big set and the total training records is 1281\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9589, Test_error: 0.5726\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9532, Test_error: 0.7310\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9627, Test_error: 0.5521\n",
      "recons_mse: 0.0709, recons_mse_added_other: 0.0647\n",
      "The length of test datasets: 287\n",
      "With big set and the total training records is 2406\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9325, Test_error: 1.0544\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9359, Test_error: 1.0154\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9347, Test_error: 0.9575\n",
      "With no big set and the total training records is 1280\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9272, Test_error: 1.1451\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9345, Test_error: 0.8757\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9367, Test_error: 0.8707\n",
      "recons_mse: 0.0734, recons_mse_added_other: 0.0466\n",
      "The length of test datasets: 266\n",
      "With big set and the total training records is 2278\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9576, Test_error: 0.4206\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9472, Test_error: 0.5143\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9502, Test_error: 0.4767\n",
      "With no big set and the total training records is 1301\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9515, Test_error: 0.4817\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9418, Test_error: 0.5025\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9523, Test_error: 0.4347\n",
      "recons_mse: 0.0725, recons_mse_added_other: 0.0513\n",
      "The length of test datasets: 293\n",
      "With big set and the total training records is 2331\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9094, Test_error: 1.0600\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9189, Test_error: 1.1332\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9077, Test_error: 1.0480\n",
      "With no big set and the total training records is 1274\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9203, Test_error: 0.9453\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9358, Test_error: 1.0016\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9377, Test_error: 0.7783\n",
      "recons_mse: 0.0716, recons_mse_added_other: 0.0636\n",
      "The length of test datasets: 297\n",
      "With big set and the total training records is 2385\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9442, Test_error: 0.9232\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9647, Test_error: 0.6117\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9550, Test_error: 0.7527\n",
      "With no big set and the total training records is 1270\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9417, Test_error: 1.0044\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9666, Test_error: 0.6292\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9566, Test_error: 0.7447\n",
      "recons_mse: 0.0742, recons_mse_added_other: 0.0476\n",
      "The length of test datasets: 286\n",
      "With big set and the total training records is 2266\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.8464, Test_error: 2.4204\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9153, Test_error: 1.3439\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9306, Test_error: 1.0677\n",
      "With no big set and the total training records is 1281\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.8641, Test_error: 2.1483\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.8244, Test_error: 3.6825\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.7890, Test_error: 4.3759\n",
      "recons_mse: 0.0735, recons_mse_added_other: 0.0496\n",
      "The length of test datasets: 322\n",
      "With big set and the total training records is 2292\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9627, Test_error: 0.4419\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9532, Test_error: 0.5373\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9449, Test_error: 0.5785\n",
      "With no big set and the total training records is 1245\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9632, Test_error: 0.4752\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9516, Test_error: 0.5425\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9606, Test_error: 0.4442\n",
      "recons_mse: 0.0733, recons_mse_added_other: 0.0643\n",
      "The length of test datasets: 294\n",
      "With big set and the total training records is 2435\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9756, Test_error: 0.4072\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9641, Test_error: 0.6058\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9761, Test_error: 0.4022\n",
      "With no big set and the total training records is 1273\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9751, Test_error: 0.4151\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9779, Test_error: 0.3946\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9763, Test_error: 0.4100\n",
      "recons_mse: 0.0739, recons_mse_added_other: 0.0509\n",
      "The length of test datasets: 313\n",
      "With big set and the total training records is 2258\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9599, Test_error: 0.8077\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9714, Test_error: 0.6084\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9705, Test_error: 0.6031\n",
      "With no big set and the total training records is 1254\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9604, Test_error: 0.7895\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9768, Test_error: 0.4966\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9703, Test_error: 0.6084\n",
      "recons_mse: 0.0708, recons_mse_added_other: 0.0547\n",
      "The length of test datasets: 305\n",
      "With big set and the total training records is 2330\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9590, Test_error: 0.7762\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9638, Test_error: 0.6682\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9764, Test_error: 0.4507\n",
      "With no big set and the total training records is 1262\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9586, Test_error: 0.7834\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9553, Test_error: 0.9835\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9607, Test_error: 0.8024\n",
      "recons_mse: 0.0731, recons_mse_added_other: 0.0614\n",
      "The length of test datasets: 265\n",
      "With big set and the total training records is 2437\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9294, Test_error: 1.2733\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9442, Test_error: 1.0489\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9306, Test_error: 1.2888\n",
      "With no big set and the total training records is 1302\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9499, Test_error: 0.9765\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9388, Test_error: 1.1526\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9273, Test_error: 1.3930\n",
      "recons_mse: 0.0710, recons_mse_added_other: 0.0479\n",
      "The length of test datasets: 314\n",
      "With big set and the total training records is 2255\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9376, Test_error: 1.1539\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9191, Test_error: 1.4834\n",
      "Dataset: CFCs, Algorithm: BGLGBM_with_big_set, Test_r2: 0.9509, Test_error: 0.8717\n",
      "With no big set and the total training records is 1253\n",
      "Dataset: CFCs, Algorithm: ETR_no_big_set, Test_r2: 0.9391, Test_error: 1.1257\n",
      "Dataset: CFCs, Algorithm: LGBM_no_big_set, Test_r2: 0.9555, Test_error: 0.7721\n",
      "Dataset: CFCs, Algorithm: BGLGBM_no_big_set, Test_r2: 0.9395, Test_error: 1.1796\n",
      "recons_mse: 0.0708, recons_mse_added_other: 0.0520\n",
      "The length of test datasets: 311\n",
      "With big set and the total training records is 2362\n",
      "Dataset: CFCs, Algorithm: ETR_with_big_set, Test_r2: 0.9443, Test_error: 0.6324\n",
      "Dataset: CFCs, Algorithm: LGBM_with_big_set, Test_r2: 0.9188, Test_error: 1.0716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14528/3859212202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The length of test datasets: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_comparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpara_grids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mk_recon_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbgk_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mgas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mk_recon_mse_added_other\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbgk_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14528/2439998821.py\u001b[0m in \u001b[0;36mmodel_comparison\u001b[1;34m(model_list, para_grids, feature_list, gas_list)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpara_grids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0mmodel_refit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mmodel_refit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14528/2439998821.py\u001b[0m in \u001b[0;36mmodel_CV\u001b[1;34m(train_x, train_y, groups, model, para_grid)\u001b[0m\n\u001b[0;32m      9\u001b[0m     result = GridSearchCV(model,para_grid,cv= out_cv.get_n_splits(groups =groups),\n\u001b[0;32m     10\u001b[0m     scoring='neg_mean_squared_error', return_train_score=True,n_jobs=-1)\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mmodel_refit\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_refit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mout_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'r2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pytorch_optuna\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with reconstruction errors\n",
    "import os\n",
    "base_feature = ['V','S','L','BET',]\n",
    "condition_feature = ['Temp(K)','Pressure']\n",
    "combin_1 = ['Vt']\n",
    "combin_2 = [\"Vmeso\"]\n",
    "combin_3 = ['Vmic']\n",
    "combin_4 = ['Vt',\"Vmeso\",]\n",
    "combin_5 = ['Vt',\"Vmic\",]\n",
    "combin_6 = ['Vt',\"Vmic\",'Vmeso',]\n",
    "combin_7 = [\"Vmic\",'Vmeso',]\n",
    "\n",
    "feature_list = [base_feature+combin_1+condition_feature]\n",
    "\n",
    "columns = ['Gas','Model_name','CV_r2','CV_mse','test_r2_total_model','test_mse_by_total_model','test_r2_separa_model','test_mse_separa_model','best_param','kpg_pca_mse','kpg_pca_mse_added_other']\n",
    "feature_set = [\"BET\",\"Vt\"]\n",
    "add_gas_list = [\"E&E\",\"Methane\",'CO2']\n",
    "target_gas_list = ['CFCs'] \n",
    "file_name = [\"BET_Vt\"]\n",
    "\n",
    "for i in range(15):\n",
    "    data_df = pred_dataset(['Ethane&Ethylene',\"CO2\",'Methane','CFCs'],feature_set= feature_set)\n",
    "    for add_gas in add_gas_list:\n",
    "        for gas in target_gas_list:\n",
    "            train_df,test_df = group_split(data_df,gas,[add_gas])\n",
    "            print('The length of test datasets: {}'.format(len(test_df)))\n",
    "            for j in range(len(feature_list)):\n",
    "                results = model_comparison(models,para_grids, feature_list[j],[gas])\n",
    "                k_recon_mse = bgk_pca(train_df[train_df[\"Label\"]==gas],test_df)\n",
    "                k_recon_mse_added_other = bgk_pca(train_df,test_df)\n",
    "                temp_results  = []\n",
    "                for ele in results:\n",
    "                    temp_results.append(ele+[k_recon_mse,k_recon_mse_added_other])\n",
    "                print(\"recons_mse: {:.4f}, recons_mse_added_other: {:.4f}\".format(k_recon_mse,k_recon_mse_added_other))\n",
    "                files_name = 'Improving_small_with_big_set'+\"_\"+gas+\"_\"+add_gas+\"_\"+file_name[j]+'_result_'+str(i)+'.csv'\n",
    "                pd.DataFrame(temp_results,columns = columns).to_csv(os.path.join('./6_Using_kernel_pca_to determine_combinable_datasets',files_name))  \n",
    "                #pd.DataFrame(results,columns = ['Gas','Algo','Train_erro','Test_error']).to_csv(os.path.join('./',files_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#file_name = ['BET_only','BET_plut_Vt',\"BET_Vmic\",\"BET_Vmeso\",'BET_Vt_Vmeso','BET_Vt_Vmic',\"BET_Vt_Vmic_meso\",\"BET_Vmic_meso\"]\n",
    "file_name = [\"BET_Vt\"]\n",
    "cal_columns= [\"CV_r2\",\"CV_mse\",\"test_r2_separa_model\",\"test_mse_separa_model\",'kpg_pca_mse','kpg_pca_mse_added_other']\n",
    "for add_gas in add_gas_list:\n",
    "    for gas in target_gas_list:\n",
    "        df_list = [] \n",
    "        for i in range(15):\n",
    "            for j in range(len(feature_list)):\n",
    "                files_name = 'Improving_small_with_big_set'+'_'+gas+\"_\"+add_gas+\"_\"+file_name[j]+'_result_'+str(i)+'.csv'\n",
    "                df_list.append(pd.read_csv(os.path.join('./6_Using_kernel_pca_to determine_combinable_datasets',files_name))[cal_columns])\n",
    "        pd.concat(df_list).groupby(level=0).mean().to_csv(os.path.join('./6_Using_kernel_pca_to determine_combinable_datasets',   add_gas+gas+'-mean.csv'))\n",
    "        pd.concat(df_list).groupby(level=0).std().to_csv(os.path.join('./6_Using_kernel_pca_to determine_combinable_datasets',add_gas+gas+'-std.csv'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cded950f7e8b102373b7ffb2d1ae075c531242f5ad58e5bbcdb99f4873d2799c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch_optuna': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
